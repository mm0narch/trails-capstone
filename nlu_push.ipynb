{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "G9f9YTFnyhGZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('combined_data.json') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "ff_F1nK2ytyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "3ac7e070-7069-4d6c-aa71-4b1e61c27985"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'combined_data.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-74c950e53f3b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'combined_data.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combined_data.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [item['text'] for item in data]\n",
        "intents = [item['intent'] for item in data]"
      ],
      "metadata": {
        "id": "s6VeR_fey1gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "NpUjB2Usy39r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_encoder = LabelEncoder()\n",
        "encoded_intents = intent_encoder.fit_transform(intents)\n",
        "num_classes_intents = len(intent_encoder.classes_)"
      ],
      "metadata": {
        "id": "gojdyOY5y50u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_types = set()\n",
        "for item in data:\n",
        "    entity_types.update(item['entities'].keys())\n",
        "entity_types = list(entity_types)"
      ],
      "metadata": {
        "id": "MnUxNHPZy8eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_labels = np.zeros((len(data), max_length, len(entity_types) + 1))  # +1 for 'O' (no entity)"
      ],
      "metadata": {
        "id": "hdTOc0tHzC8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(data):\n",
        "    words = item['text'].split()\n",
        "    sequence = sequences[i]\n",
        "    for entity, value in item['entities'].items():\n",
        "        entity_type_idx = entity_types.index(entity)\n",
        "        value_words = value.split()\n",
        "        for j in range(len(words)):\n",
        "            if words[j:j+len(value_words)] == value_words:\n",
        "                for k in range(len(value_words)):\n",
        "                    if j + k < max_length:\n",
        "                        entity_labels[i, j + k, entity_type_idx + 1] = 1  # +1 to account for 'O'\n",
        "                break"
      ],
      "metadata": {
        "id": "mpWBMhR5zFNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_labels = np.argmax(entity_labels, axis=-1)"
      ],
      "metadata": {
        "id": "HJxa1a6ezIIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_intent_train, y_intent_val, y_entity_train, y_entity_val = train_test_split(\n",
        "    padded_sequences, encoded_intents, entity_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "XSCcwJeFzkbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Bidirectional"
      ],
      "metadata": {
        "id": "2VQO38yIzmY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "input_text = Input(shape=(max_length,))\n",
        "embedding = Embedding(input_dim=len(word_index) + 1, output_dim=128)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(units=64, return_sequences=True))(embedding)\n",
        "lstm = LSTM(units=64, return_sequences=True)(embedding)"
      ],
      "metadata": {
        "id": "ZEGHYigozq5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_dense = Dense(units=64, activation='relu')(lstm[:, -1, :])\n",
        "intent_output = Dense(units=num_classes_intents, activation='softmax', name='intent_output')(intent_dense)"
      ],
      "metadata": {
        "id": "wT2GMM_kzslb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_output = TimeDistributed(Dense(units=len(entity_types) + 1, activation='softmax'), name='entity_output')(lstm)"
      ],
      "metadata": {
        "id": "FNRg8Xeazudb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=input_text, outputs=[intent_output, entity_output])\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'intent_output': 'sparse_categorical_crossentropy', 'entity_output': 'sparse_categorical_crossentropy'},\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv98SB2fzwff",
        "outputId": "a5be5abf-9dea-4871-8025-9917941db778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 9)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 9, 128)               14720     ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 9, 64)                49408     ['embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 64)                   0         ['lstm_3[0][0]']              \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 64)                   4160      ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " intent_output (Dense)       (None, 3)                    195       ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " entity_output (TimeDistrib  (None, 9, 2)                 130       ['lstm_3[0][0]']              \n",
            " uted)                                                                                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 68613 (268.02 KB)\n",
            "Trainable params: 68613 (268.02 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,\n",
        "                    {'intent_output': y_intent_train, 'entity_output': y_entity_train},\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_val, {'intent_output': y_intent_val, 'entity_output': y_entity_val}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ5Tgq-Azyzd",
        "outputId": "841e773b-2752-4afd-9e20-23e87db873d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 5s 61ms/step - loss: 1.6744 - intent_output_loss: 1.0942 - entity_output_loss: 0.5802 - intent_output_accuracy: 0.3656 - entity_output_accuracy: 0.8556 - val_loss: 1.5340 - val_intent_output_loss: 1.0894 - val_entity_output_loss: 0.4446 - val_intent_output_accuracy: 0.3462 - val_entity_output_accuracy: 0.8547\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 1.4681 - intent_output_loss: 1.0751 - entity_output_loss: 0.3930 - intent_output_accuracy: 0.4391 - entity_output_accuracy: 0.8635 - val_loss: 1.4208 - val_intent_output_loss: 1.0363 - val_entity_output_loss: 0.3845 - val_intent_output_accuracy: 0.5692 - val_entity_output_accuracy: 0.8547\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 0s 15ms/step - loss: 1.2299 - intent_output_loss: 0.8749 - entity_output_loss: 0.3550 - intent_output_accuracy: 0.5977 - entity_output_accuracy: 0.8640 - val_loss: 0.9556 - val_intent_output_loss: 0.6063 - val_entity_output_loss: 0.3493 - val_intent_output_accuracy: 0.7923 - val_entity_output_accuracy: 0.8547\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.6774 - intent_output_loss: 0.3434 - entity_output_loss: 0.3339 - intent_output_accuracy: 0.9226 - entity_output_accuracy: 0.8652 - val_loss: 0.4645 - val_intent_output_loss: 0.1290 - val_entity_output_loss: 0.3355 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.8573\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.3690 - intent_output_loss: 0.0589 - entity_output_loss: 0.3101 - intent_output_accuracy: 0.9981 - entity_output_accuracy: 0.8670 - val_loss: 0.3142 - val_intent_output_loss: 0.0217 - val_entity_output_loss: 0.2925 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.8598\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 0.2848 - intent_output_loss: 0.0188 - entity_output_loss: 0.2660 - intent_output_accuracy: 0.9981 - entity_output_accuracy: 0.8717 - val_loss: 0.2561 - val_intent_output_loss: 0.0092 - val_entity_output_loss: 0.2469 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.8752\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.2366 - intent_output_loss: 0.0123 - entity_output_loss: 0.2243 - intent_output_accuracy: 0.9981 - entity_output_accuracy: 0.8882 - val_loss: 0.2121 - val_intent_output_loss: 0.0056 - val_entity_output_loss: 0.2065 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.9000\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 0.1966 - intent_output_loss: 0.0071 - entity_output_loss: 0.1895 - intent_output_accuracy: 0.9981 - entity_output_accuracy: 0.9179 - val_loss: 0.1803 - val_intent_output_loss: 0.0054 - val_entity_output_loss: 0.1748 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.9402\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 0s 12ms/step - loss: 0.1675 - intent_output_loss: 0.0032 - entity_output_loss: 0.1643 - intent_output_accuracy: 1.0000 - entity_output_accuracy: 0.9443 - val_loss: 0.1544 - val_intent_output_loss: 0.0041 - val_entity_output_loss: 0.1503 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.9530\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 0s 14ms/step - loss: 0.1467 - intent_output_loss: 0.0018 - entity_output_loss: 0.1449 - intent_output_accuracy: 1.0000 - entity_output_accuracy: 0.9489 - val_loss: 0.1340 - val_intent_output_loss: 0.0025 - val_entity_output_loss: 0.1315 - val_intent_output_accuracy: 1.0000 - val_entity_output_accuracy: 0.9573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, intent_loss, entity_loss, intent_acc, entity_acc = model.evaluate(X_val,\n",
        "                                                                        {'intent_output': y_intent_val, 'entity_output': y_entity_val})\n",
        "\n",
        "print(f\"Loss: {loss}, Intent Accuracy: {intent_acc}, Entity Accuracy: {entity_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULVMoO1qz077",
        "outputId": "f12bf1c8-d26b-44fd-ba7a-456d796c090b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1340 - intent_output_loss: 0.0025 - entity_output_loss: 0.1315 - intent_output_accuracy: 1.0000 - entity_output_accuracy: 0.9573\n",
            "Loss: 0.13399218022823334, Intent Accuracy: 1.0, Entity Accuracy: 0.9572649598121643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('nlu_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Qx4dfd0DP6",
        "outputId": "22588990-4b4b-4d6b-ad46-dc7b828bbf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "model = load_model('nlu_model.h5')\n",
        "\n",
        "# Function to predict intent and entities\n",
        "def predict_nlu(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    intent_pred, entity_pred = model.predict(padded_sequence)\n",
        "\n",
        "    intent = intent_encoder.inverse_transform([np.argmax(intent_pred)])\n",
        "    entities = np.argmax(entity_pred, axis=-1)\n",
        "\n",
        "    entity_list = []\n",
        "    for i, idx in enumerate(entities[0]):\n",
        "        if idx > 0:\n",
        "            entity = entity_types[idx - 1]\n",
        "            entity_value = text.split()[i]\n",
        "            entity_list.append((entity, entity_value))\n",
        "\n",
        "    return intent[0], entity_list\n",
        "\n",
        "# Example usage\n",
        "text = \"Find hidden gems in Legian region.\"\n",
        "intent, entities = predict_nlu(text)\n",
        "print(f\"Intent: {intent}, Entities: {entities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gzOtr3X0Prs",
        "outputId": "88fb4866-93e7-4043-b8b5-ca74bc062d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 511ms/step\n",
            "Intent: FindTouristAttraction, Entities: [('LOCATION', 'Legian')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "intent_counts = Counter([item['intent'] for item in data])\n",
        "print(intent_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eQ7MisB0Skl",
        "outputId": "f38c9d10-e180-483f-bdd1-c05d10d69510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'FindTouristAttraction': 228, 'FindRestaurant': 212, 'FindHotel': 207})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load data from all three JSON files\n",
        "with open('hotel.json', 'r') as f:\n",
        "    hotel_data = json.load(f)\n",
        "\n",
        "with open('restaurants.json', 'r') as f:\n",
        "    restaurants_data = json.load(f)\n",
        "\n",
        "with open('touristattraction.json', 'r') as f:\n",
        "    touristattraction_data = json.load(f)\n",
        "\n",
        "# Combine data into a single list\n",
        "combined_data = hotel_data + restaurants_data + touristattraction_data\n",
        "\n",
        "# Save combined data to a new JSON file\n",
        "output_file = 'combined_data.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(combined_data, f, indent=2)\n",
        "\n",
        "print(\"Combined data has been saved to 'combined_data.json'\")"
      ],
      "metadata": {
        "id": "e-6lCZkr1MYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b475cc3-b7f6-4dae-bf20-80a480d91ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined data has been saved to 'combined_data.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8GcB9YE9BMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}